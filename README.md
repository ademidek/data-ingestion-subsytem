# The Data Ingestion Subsystem ðŸ”Œ

## Application Overview ðŸ“
This application is a **Data Ingestion Subsystem**, designed to collect and organize data from different sources into a unified, structured environment. It is a foundational component of modern data engineering pipelines, responsible for ensuring data is **accurate, clean, and reliable** before downstream analytics or machine learning use.

This subsystem uses **Python** and **PostgreSQL** to read (extract), validate, clean (transform), and load data for later use in analytics or warehousing, similar to other ETL pipelines.

The goal of this application is to analyze and visualize potential correlations between esophageal cancer and several key risk factors, including smoking history, alcohol intake, and body mass index (BMI). By building a structured ETL pipeline, the project transforms raw medical datasets into clean, validated, and analysis-ready information.

In addition to lifestyle factors, the application also explores the clinical progression often associated with esophageal cancer. This includes examining the relationship between acid reflux, Barrettâ€™s Esophagus, and other intermediary conditions that may increase the likelihood of developing esophageal cancer. Through data cleaning, validation, and intuitive visualizations, the system aims to highlight meaningful patterns that could support risk assessment, research, or early-warning insights.

---

## Application Goals ðŸ¥…

This project was designed with two goals in mind:

1. Educational â€” To demonstrate how medical datasets can be ingested, validated, cleaned, and transformed through a modular ETL pipeline. This includes applying data quality rules, building reusable components, and producing structured outputs suitable for analysis.

2. Analytical â€” To investigate how behavioral and clinical factorsâ€”such as smoking history, alcohol consumption, BMI, reflux, and Barrettâ€™s Esophagusâ€”may correlate with the development of esophageal cancer. The project helps surface meaningful trends and relationships through exploratory analysis and visualization.

## Project Structure
```
data_ingestion_pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ readers/
â”‚   â”‚   â”œâ”€â”€ csv_reader.py
â”‚   â”‚   â”œâ”€â”€ json_reader.py
â”‚   â”‚   â””â”€â”€ api_reader.py
â”‚   â”œâ”€â”€ validate.py
â”‚   â”œâ”€â”€ clean.py
â”‚   â”œâ”€â”€ load.py
â”‚   â”œâ”€â”€ rules.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ sources.yml
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Esophageal_Dataset.csv
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_validate.py
â”‚   â””â”€â”€ test_load.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

Personal notes before finishing:

-list the technologies im using (the tech stack), mention logging
-add some steps to so someone can locally run and test the program, whatever necessary commands
-try and clone the project and see whatever steps you would need to run
-try and test the validation and cleaning logic, anything transformation
-enriching the data is part of transformation, try and refactor the code to enrich the data separate from the validation
-one test could be testing to see if the trigger for adding a column works by computing BMI where height > 0 and both height & weight are present
-modularize my code more, break things down into functions so they can be tested.
-shorten the validation code logic and break it up into smaller functions.
-create oop sort of method you have your class and your methods